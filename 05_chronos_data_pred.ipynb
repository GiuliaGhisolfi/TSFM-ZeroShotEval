{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eba777",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91ae85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'timestamp', 'target', 'im_0'],\n",
       "        num_rows: 100014\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'timestamp', 'target'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'timestamp', 'target'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "CHRONOS = \"autogluon/chronos_datasets\"\n",
    "DATASET_CHRONOS = [\n",
    "    \"dominick\",\n",
    "    \"ercot\",\n",
    "    \"exchange_rate\",\n",
    "    #\"monash_m3_monthly\"\n",
    "]\n",
    "\n",
    "for dataset in DATASET_CHRONOS:\n",
    "    ds = load_dataset(CHRONOS, dataset, trust_remote_code=True)\n",
    "    display(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd1660",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a6baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MSE,\n",
    "    MAE,\n",
    "    MASE,\n",
    "    MAPE,\n",
    "    SMAPE,\n",
    "    MSIS,\n",
    "    RMSE,\n",
    "    NRMSE,\n",
    "    ND,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcef99e",
   "metadata": {},
   "source": [
    "### Results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b968571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"chronos_data_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f589b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f18dcf",
   "metadata": {},
   "source": [
    "### Chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline, ForecastType\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    quantile_levels: Optional[List[float]] = None\n",
    "    forecast_keys: List[str] = field(init=False)\n",
    "    statsforecast_keys: List[str] = field(init=False)\n",
    "    intervals: Optional[List[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.forecast_keys = [\"mean\"]\n",
    "        self.statsforecast_keys = [\"mean\"]\n",
    "        if self.quantile_levels is None:\n",
    "            self.intervals = None\n",
    "            return\n",
    "\n",
    "        intervals = set()\n",
    "\n",
    "        for quantile_level in self.quantile_levels:\n",
    "            interval = round(200 * (max(quantile_level, 1 - quantile_level) - 0.5))\n",
    "            intervals.add(interval)\n",
    "            side = \"hi\" if quantile_level > 0.5 else \"lo\"\n",
    "            self.forecast_keys.append(str(quantile_level))\n",
    "            self.statsforecast_keys.append(f\"{side}-{interval}\")\n",
    "\n",
    "        self.intervals = sorted(intervals)\n",
    "\n",
    "class ChronosPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        num_samples: int,\n",
    "        prediction_length: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(\"prediction_length:\", prediction_length)\n",
    "        self.pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            model_path,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        pipeline = self.pipeline\n",
    "        predict_kwargs = (\n",
    "            {\"num_samples\": self.num_samples}\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES\n",
    "            else {}\n",
    "        )\n",
    "        while True:\n",
    "            try:\n",
    "                # Generate forecast samples\n",
    "                forecast_outputs = []\n",
    "                for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "                    context = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "                    forecast_outputs.append(\n",
    "                        pipeline.predict(\n",
    "                            context,\n",
    "                            prediction_length=self.prediction_length,\n",
    "                            **predict_kwargs,\n",
    "                        ).numpy()\n",
    "                    )\n",
    "                forecast_outputs = np.concatenate(forecast_outputs)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\n",
    "                    f\"OutOfMemoryError at batch_size {batch_size}, reducing to {batch_size // 2}\"\n",
    "                )\n",
    "                batch_size //= 2\n",
    "\n",
    "        # Convert forecast samples into gluonts Forecast objects\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES:\n",
    "                forecasts.append(\n",
    "                    SampleForecast(samples=item, start_date=forecast_start_date)\n",
    "                )\n",
    "            elif pipeline.forecast_type == ForecastType.QUANTILES:\n",
    "                forecasts.append(\n",
    "                    QuantileForecast(\n",
    "                        forecast_arrays=item,\n",
    "                        forecast_keys=list(map(str, pipeline.quantiles)),\n",
    "                        start_date=forecast_start_date,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return forecasts\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e86c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"chronos_bolt_small\" # TODO: change to \"chronos_t5_base\" for the original Chronos model\n",
    "\n",
    "model_path=\"amazon/chronos-bolt-small\",\n",
    "# TODO: use \"amazon/chronos-t5-base\" for the corresponding original Chronos model\n",
    "# \"amazon/chronos-bolt-tiny\", \"amazon/chronos-bolt-mini\", \"amazon/chronos-bolt-small\", \"amazon/chronos-bolt-base\",\n",
    "# \"amazon/chronos-t5-tiny\", \"amazon/chronos-t5-mini\", \"amazon/chronos-t5-small\",\n",
    "# \"amazon/chronos-t5-base\", \"amazon/chronos-t5-large\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ccf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'timestamp', 'target'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5befed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model import evaluate_model, evaluate_forecasts\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "for ds_name in DATASET_CHRONOS:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        dataset = load_dataset(CHRONOS, ds_name, trust_remote_code=True)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        prediction_length = dataset[\"train\"].metadata.prediction_length\n",
    "        dataset_properties_map = {\n",
    "            \"ercot\": {\"num_variates\": 1},\n",
    "            \"exchange_rate\": {\"num_variates\": 8},\n",
    "            \"dominick\": {\"num_variates\": 1},\n",
    "            # \"monash_m3_monthly\": {\"num_variates\": 1},\n",
    "        }\n",
    "\n",
    "        print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "        predictor = ChronosPredictor(\n",
    "            model_path=model_path,\n",
    "            num_samples=20,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            # Change device_map to \"cpu\" to run on CPU or \"cuda\" to run on GPU\n",
    "            device_map=\"cpu\",\n",
    "        )\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = predictor.predict(\n",
    "            test_data_input=dataset[\"train\"],\n",
    "            batch_size=512,\n",
    "        )\n",
    "\n",
    "        # Evaluate the predictions\n",
    "        res = evaluate_forecasts(\n",
    "            predictions=predictions,\n",
    "            test_data=dataset[\"train\"],\n",
    "            metrics=metrics,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            season_length=season_length,\n",
    "            quantiles=[0.5],\n",
    "            num_samples=20,\n",
    "            num_quantiles=10,\n",
    "            num_samples_per_quantile=20,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_name,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    ds_name,\n",
    "                    dataset_properties_map[ds_name][\"num_variates\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ae471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

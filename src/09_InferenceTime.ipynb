{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotenv\n",
    "!pip install gluonts\n",
    "!pip install --upgrade datasets\n",
    "!pip install utilsforecast\n",
    "!pip install lightning\n",
    "!pip install jaxtyping\n",
    "!pip install hydra-core\n",
    "!pip install --upgrade transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462baf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/GiuliaGhisolfi/TSFM-ZeroShotEval\n",
    "%cd TSFM-ZeroShotEval/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6acae5",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4470c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "short_datasets = \"solar/10T solar/H solar/D solar/W jena_weather/10T jena_weather/H jena_weather/D \" \\\n",
    "\"bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application \" \\\n",
    "\"bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "\n",
    "med_long_datasets = \"solar/10T solar/H jena_weather/10T jena_weather/H \" \\\n",
    "\"bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"data/dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_data import load_gift_data\n",
    "\n",
    "load_gift_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14babb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHRONOS_DATASET_NAME = [\"exchange_rate\", \"ercot\", \"dominick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974c211",
   "metadata": {},
   "source": [
    "### Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "output_file_name = \"inference_time.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75013430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"model\",\n",
    "            \"dataset\",\n",
    "            \"trial\",\n",
    "            \"inference_time\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "            \"prediction_length\",\n",
    "            \"frequency\"\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acf608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, ds_name, i, end, start, domain, num_variates, prediction_length, frequency):\n",
    "    # Append the results to the CSV file\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                model_name,\n",
    "                ds_name,\n",
    "                i,\n",
    "                end-start,\n",
    "                domain,\n",
    "                num_variates,\n",
    "                prediction_length,\n",
    "                frequency\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(f\"Results for {ds_name} have been written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea15ae4",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gift_eval.data import Dataset\n",
    "from utils.load_chronos_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3323c6",
   "metadata": {},
   "source": [
    "### Chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e03c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHRONOS_MODEL_NAME = [\"chronos_bolt_tiny\", \"chronos_bolt_mini\", \"chronos_bolt_small\", \"chronos_bolt_base\"]\n",
    "\n",
    "CHRONOS_MODEL = [\"amazon/chronos-bolt-tiny\", \"amazon/chronos-bolt-mini\",\n",
    "    \"amazon/chronos-bolt-small\", \"amazon/chronos-bolt-base\"]\n",
    "# \"amazon/chronos-t5-tiny\", \"amazon/chronos-t5-mini\", \"amazon/chronos-t5-small\",\n",
    "# \"amazon/chronos-t5-base\", \"amazon/chronos-t5-large\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chronos_predictor import ChronosPredictor\n",
    "\n",
    "for model_name, model_path in zip(CHRONOS_MODEL_NAME, CHRONOS_MODEL):\n",
    "    # Chronos Datasets\n",
    "    for ds_name in CHRONOS_DATASET_NAME:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            ds_config = f\"{ds_name}/{term}\"\n",
    "            for i in range(10):\n",
    "\n",
    "                dataset, prediction_length, frequency, domain, num_variates = load_data(ds_name, term)\n",
    "\n",
    "                # Init predictor\n",
    "                predictor = ChronosPredictor(\n",
    "                    model_path=model_path,\n",
    "                    num_samples=20,\n",
    "                    prediction_length=prediction_length,\n",
    "                    # Change device_map to \"cpu\" to run on CPU or \"cuda\" to run on GPU\n",
    "                    device_map=\"cuda\",\n",
    "                )\n",
    "\n",
    "                start = time.time()\n",
    "                forecasts = predictor.predict(dataset.input)\n",
    "                end = time.time()\n",
    "\n",
    "                # Append the results to the CSV file\n",
    "                save_results(model_name, ds_config, i, end, start, domain, num_variates, prediction_length, frequency)\n",
    "\n",
    "    # GIFT-Eval Datasets\n",
    "    for ds_name in all_datasets:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            if (\n",
    "                term == \"medium\" or term == \"long\"\n",
    "            ) and ds_name not in med_long_datasets.split():\n",
    "                continue\n",
    "\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key = ds_name.split(\"/\")[0]\n",
    "                ds_freq = ds_name.split(\"/\")[1]\n",
    "                ds_key = ds_key.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "            ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "            for i in range(10):\n",
    "                # Initialize the dataset\n",
    "                to_univariate = (\n",
    "                    False\n",
    "                    if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "                    else True\n",
    "                )\n",
    "                dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "                prediction_length = dataset.prediction_length\n",
    "                frequency = ds_freq\n",
    "                domain  = dataset_properties_map[ds_key][\"domain\"],\n",
    "                num_variates = dataset_properties_map[ds_key][\"num_variates\"],\n",
    "\n",
    "                # Init predictor\n",
    "                predictor = ChronosPredictor(\n",
    "                    model_path=model_path,\n",
    "                    num_samples=20,\n",
    "                    prediction_length=prediction_length,\n",
    "                    # Change device_map to \"cpu\" to run on CPU or \"cuda\" to run on GPU\n",
    "                    device_map=\"cuda\",\n",
    "                )\n",
    "\n",
    "                start = time.time()\n",
    "                forecasts = predictor.predict(dataset.test_data.input)\n",
    "                end = time.time()\n",
    "\n",
    "                # Append the results to the CSV file\n",
    "                save_results(model_name, ds_config, i, end, start, domain, num_variates, prediction_length, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0df99",
   "metadata": {},
   "source": [
    "### Moirai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOIRAI_MODEL_NAME = [\"moirai_small\", \"moirai_base\", \"moirai_large\"]\n",
    "\n",
    "MOIRAI_MODEL = [\"Salesforce/moirai-1.1-R-small\", \"Salesforce/moirai-1.1-R-base\", \n",
    "    \"Salesforce/moirai-1.1-R-large\"]\n",
    "#\"Salesforce/moirai-moe-1.0-R-base\", \"Salesforce/moirai-moe-1.0-R-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.moirai_predictor import load_predictor\n",
    "\n",
    "for model_name, model_path in zip(MOIRAI_MODEL_NAME, MOIRAI_MODEL):\n",
    "    # Chronos Datasets\n",
    "    for ds_name in CHRONOS_DATASET_NAME:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            ds_config = f\"{ds_name}/{term}\"\n",
    "            for i in range(10):\n",
    "\n",
    "                dataset, prediction_length, frequency, domain, num_variates = load_data(ds_name, term)\n",
    "\n",
    "                # Init predictor\n",
    "                predictor = load_predictor(model_path, prediction_length, num_variates)\n",
    "\n",
    "                start = time.time()\n",
    "                forecasts = predictor.predict(dataset.input)\n",
    "                end = time.time()\n",
    "\n",
    "                # Append the results to the CSV file\n",
    "                save_results(model_name, ds_config, i, end, start, domain, num_variates, prediction_length, frequency)\n",
    "\n",
    "    # GIFT-Eval Datasets\n",
    "    for ds_name in all_datasets:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            if (\n",
    "                term == \"medium\" or term == \"long\"\n",
    "            ) and ds_name not in med_long_datasets.split():\n",
    "                continue\n",
    "\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key = ds_name.split(\"/\")[0]\n",
    "                ds_freq = ds_name.split(\"/\")[1]\n",
    "                ds_key = ds_key.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "            ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "            for i in range(10):\n",
    "                # Initialize the dataset\n",
    "                to_univariate = (\n",
    "                    False\n",
    "                    if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "                    else True\n",
    "                )\n",
    "                dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "                prediction_length = dataset.prediction_length\n",
    "                frequency = ds_freq\n",
    "                domain  = dataset_properties_map[ds_key][\"domain\"],\n",
    "                num_variates = dataset_properties_map[ds_key][\"num_variates\"],\n",
    "\n",
    "                # Init predictor\n",
    "                predictor = load_predictor(model_path, prediction_length, num_variates)\n",
    "\n",
    "                start = time.time()\n",
    "                forecasts = predictor.predict(dataset.test_data.input)\n",
    "                end = time.time()\n",
    "\n",
    "                # Append the results to the CSV file\n",
    "                save_results(model_name, ds_config, i, end, start, domain, num_variates, prediction_length, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced41fae",
   "metadata": {},
   "source": [
    "### TimesFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb570b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESFM_MODEL_NAME = [\"timesfm2\", \"timesfm1\"]\n",
    "\n",
    "TIMESFM_MODEL = [\"google/timesfm-2.0-500m-pytorch\", \"google/timesfm-1.0-200m-pytorch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.timesfm_predictor import load_predictor\n",
    "\n",
    "for model_name, model_path in zip(TIMESFM_MODEL_NAME, TIMESFM_MODEL):\n",
    "    # Chronos Datasets\n",
    "    for ds_name in CHRONOS_DATASET_NAME:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            ds_config = f\"{ds_name}/{term}\"\n",
    "            for i in range(10):\n",
    "\n",
    "                dataset, prediction_length, frequency, domain, num_variates = load_data(ds_name, term)\n",
    "\n",
    "                # Init predictor\n",
    "                predictor = load_predictor(model_path, prediction_length, frequency)\n",
    "\n",
    "                start = time.time()\n",
    "                forecasts = predictor.predict(dataset.input)\n",
    "                end = time.time()\n",
    "\n",
    "                # Append the results to the CSV file\n",
    "                save_results(model_name, ds_config, i, end, start, domain, num_variates, prediction_length, frequency)\n",
    "\n",
    "    # GIFT-Eval Datasets\n",
    "    for ds_name in all_datasets:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            if (\n",
    "                term == \"medium\" or term == \"long\"\n",
    "            ) and ds_name not in med_long_datasets.split():\n",
    "                continue\n",
    "\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key = ds_name.split(\"/\")[0]\n",
    "                ds_freq = ds_name.split(\"/\")[1]\n",
    "                ds_key = ds_key.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "            ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "            for i in range(10):\n",
    "                # Initialize the dataset\n",
    "                to_univariate = (\n",
    "                    False\n",
    "                    if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "                    else True\n",
    "                )\n",
    "                dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "                prediction_length = dataset.prediction_length\n",
    "                frequency = ds_freq\n",
    "                domain  = dataset_properties_map[ds_key][\"domain\"],\n",
    "                num_variates = dataset_properties_map[ds_key][\"num_variates\"],\n",
    "\n",
    "                # Init predictor\n",
    "                predictor = load_predictor(model_path, prediction_length, frequency)\n",
    "\n",
    "                start = time.time()\n",
    "                forecasts = predictor.predict(dataset.test_data.input)\n",
    "                end = time.time()\n",
    "\n",
    "                # Append the results to the CSV file\n",
    "                save_results(model_name, ds_config, i, end, start, domain, num_variates, prediction_length, frequency)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

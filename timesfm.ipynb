{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running timesfm models on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run timesfm-2.0 on gift-eval.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install timesfm package in a python 3.10.x env:\n",
    "``\n",
    "pip install timesfm[pax]\n",
    "``\n",
    "\n",
    "You can also try the torch version in a python 3.11.x env:\n",
    "``\n",
    "pip install timesfm[torch]\n",
    "``\n",
    "\n",
    "After that you can install the gift-eval package:\n",
    "``\n",
    "pip install -e .\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "short_datasets = \"m4_weekly\"\n",
    "\n",
    "# med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "med_long_datasets = \"bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimesFM Predictor\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets first load the timesfm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.core.strings' has no attribute 'StringMethods'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maaaaa\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtimesfm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tfm \u001b[38;5;241m=\u001b[39m timesfm\u001b[38;5;241m.\u001b[39mTimesFm(\n\u001b[0;32m      4\u001b[0m     hparams\u001b[38;5;241m=\u001b[39mtimesfm\u001b[38;5;241m.\u001b[39mTimesFmHparams(\n\u001b[0;32m      5\u001b[0m         backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m         huggingface_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/timesfm-2.0-500m-jax\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Documenti\\VS_Code\\tesi\\TSFM-ZeroShotEval\\aaaaa\\__init__.py:21\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maaaaa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimesfm_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (TimesFmBase, TimesFmCheckpoint, TimesFmHparams,\n\u001b[0;32m     22\u001b[0m                                 freq_map)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maaaaa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimesfm_jax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimesFmJax \u001b[38;5;28;01mas\u001b[39;00m TimesFm\n",
      "File \u001b[1;32mc:\\Documenti\\VS_Code\\tesi\\TSFM-ZeroShotEval\\aaaaa\\timesfm_base.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilsforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_future_dataframe\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xreg_lib\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\utilsforecast\\processing.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffsets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOffset\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame, Series, pl, pl_DataFrame, pl_Series\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilsforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     _is_dt_dtype,\n\u001b[0;32m     24\u001b[0m     _is_int_dtype,\n\u001b[0;32m     25\u001b[0m     ensure_shallow_copy,\n\u001b[0;32m     26\u001b[0m     validate_format,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# %% ../nbs/processing.ipynb 5\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\utilsforecast\\compat.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame \u001b[38;5;28;01mas\u001b[39;00m DaskDataFrame\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pyarrow_compat\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backends, dispatch, rolling\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     DataFrame,\n\u001b[0;32m      7\u001b[0m     Index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     to_timedelta,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Aggregation\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\backends.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m percentile_lookup\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpercentile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _percentile\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame, Index, Scalar, Series, _Frame\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     categorical_dtype_dispatch,\n\u001b[0;32m     23\u001b[0m     concat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     union_categoricals_dispatch,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_array_nonempty, make_scalar\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblockwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Blockwise, BlockwiseDep, BlockwiseDepDict, blockwise\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m globalmethod\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m methods\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PANDAS_GT_140, PANDAS_GT_150\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CachedAccessor, DatetimeAccessor, StringAccessor\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\methods.py:22\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#  preserve compatibility while moving dispatch objects\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     concat,\n\u001b[0;32m     13\u001b[0m     concat_dispatch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     union_categoricals,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_dataframe_like, is_index_like, is_series_like\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# cuDF may try to import old dispatch functions\u001b[39;00m\n\u001b[0;32m     25\u001b[0m hash_df \u001b[38;5;241m=\u001b[39m hash_object_dispatch\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_scheduler, is_dask_collection\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_deps\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 register pandas extension types\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     _dtypes,\n\u001b[0;32m     20\u001b[0m     methods,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PANDAS_GT_110, PANDAS_GT_120, tm  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa : F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     make_meta,\n\u001b[0;32m     25\u001b[0m     make_meta_obj,\n\u001b[0;32m     26\u001b[0m     meta_nonempty,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\_dtypes.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_array_nonempty, make_scalar\n\u001b[0;32m      6\u001b[0m \u001b[38;5;129m@make_array_nonempty\u001b[39m\u001b[38;5;241m.\u001b[39mregister(pd\u001b[38;5;241m.\u001b[39mDatetimeTZDtype)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_\u001b[39m(dtype):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39marray([pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;241m1\u001b[39m), pd\u001b[38;5;241m.\u001b[39mNaT], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\extensions.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mSupport for pandas ExtensionArray in dask.dataframe.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mSee :ref:`extensionarrays` for more.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     register_dataframe_accessor,\n\u001b[0;32m      8\u001b[0m     register_index_accessor,\n\u001b[0;32m      9\u001b[0m     register_series_accessor,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dispatch\n\u001b[0;32m     13\u001b[0m make_array_nonempty \u001b[38;5;241m=\u001b[39m Dispatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_array_nonempty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\accessor.py:190\u001b[0m\n\u001b[0;32m    129\u001b[0m     _accessor_methods \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mceil\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtz_localize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m     _accessor_properties \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    187\u001b[0m     )\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStringAccessor\u001b[39;00m(Accessor):\n\u001b[0;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Accessor object for string properties of the Series values.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    >>> s.str.lower()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     _accessor_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\anaconda3\\lib\\site-packages\\dask\\dataframe\\accessor.py:276\u001b[0m, in \u001b[0;36mStringAccessor\u001b[1;34m()\u001b[0m\n\u001b[0;32m    272\u001b[0m         meta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_series\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_map(method, pat\u001b[38;5;241m=\u001b[39mpat, n\u001b[38;5;241m=\u001b[39mn, expand\u001b[38;5;241m=\u001b[39mexpand, meta\u001b[38;5;241m=\u001b[39mmeta)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;129m@derived_from\u001b[39m(\n\u001b[1;32m--> 276\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringMethods\u001b[49m,\n\u001b[0;32m    277\u001b[0m     inconsistencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``expand=True`` with unknown ``n`` will raise a ``NotImplementedError``\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    278\u001b[0m )\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, pat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Known inconsistencies: ``expand=True`` with unknown ``n`` will raise a ``NotImplementedError``.\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, pat\u001b[38;5;241m=\u001b[39mpat, n\u001b[38;5;241m=\u001b[39mn, expand\u001b[38;5;241m=\u001b[39mexpand)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas.core.strings' has no attribute 'StringMethods'"
     ]
    }
   ],
   "source": [
    "import timesfm as timesfm\n",
    "\n",
    "tfm = timesfm.TimesFm(\n",
    "    hparams=timesfm.TimesFmHparams(\n",
    "        backend=\"gpu\",\n",
    "        per_core_batch_size=32,\n",
    "        num_layers=50,\n",
    "        horizon_len=128,\n",
    "        context_len=2048,\n",
    "        use_positional_embedding=False,\n",
    "        output_patch_len=128,\n",
    "    ),\n",
    "    checkpoint=timesfm.TimesFmCheckpoint(\n",
    "        huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n",
    ")\n",
    "\n",
    "# If you are using the pytorch version:\n",
    "# tfm = timesfm.TimesFm(\n",
    "#     hparams=timesfm.TimesFmHparams(\n",
    "#         backend=\"gpu\",\n",
    "#         per_core_batch_size=32,\n",
    "#         num_layers=50,\n",
    "#         horizon_len=128,\n",
    "#         context_len=2048,\n",
    "#         use_positional_embedding=False,\n",
    "#         output_patch_len=128,\n",
    "#     ),\n",
    "#     checkpoint=timesfm.TimesFmCheckpoint(\n",
    "#         huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "\n",
    "\n",
    "class TimesFmPredictor:\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      tfm,\n",
    "      prediction_length: int,\n",
    "      ds_freq: str,\n",
    "      *args,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    self.tfm = tfm\n",
    "    self.prediction_length = prediction_length\n",
    "    if self.prediction_length > self.tfm.horizon_len:\n",
    "      self.tfm.horizon_len = (\n",
    "          (self.prediction_length + self.tfm.output_patch_len - 1) //\n",
    "          self.tfm.output_patch_len) * self.tfm.output_patch_len\n",
    "      print('Jitting for new prediction length.')\n",
    "    self.freq = timesfm.freq_map(ds_freq)\n",
    "\n",
    "  def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "    forecast_outputs = []\n",
    "    for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "      context = []\n",
    "      for entry in batch:\n",
    "        arr = np.array(entry[\"target\"])\n",
    "        context.append(arr)\n",
    "      freqs = [self.freq] * len(context)\n",
    "      _, full_preds = self.tfm.forecast(context, freqs, normalize=True)\n",
    "      full_preds = full_preds[:, 0:self.prediction_length, 1:]\n",
    "      forecast_outputs.append(full_preds.transpose((0, 2, 1)))\n",
    "    forecast_outputs = np.concatenate(forecast_outputs)\n",
    "\n",
    "    # Convert forecast samples into gluonts Forecast objects\n",
    "    forecasts = []\n",
    "    for item, ts in zip(forecast_outputs, test_data_input):\n",
    "      forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "      forecasts.append(\n",
    "          QuantileForecast(\n",
    "              forecast_arrays=item,\n",
    "              forecast_keys=list(map(str, self.tfm.quantiles)),\n",
    "              start_date=forecast_start_date,\n",
    "          ))\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/timesfm_2_0_500m` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering all dataset settings from lowest to highest prediction length to minimize the number of jittings. This is not necessary for the pytorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "all_ds_tuples = []\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "  ds_key = ds_name.split(\"/\")[0]\n",
    "  print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "  terms = [\"short\", \"medium\", \"long\"]\n",
    "  for term in terms:\n",
    "    if (term == \"medium\" or\n",
    "        term == \"long\") and ds_name not in med_long_datasets.split():\n",
    "      continue\n",
    "\n",
    "    if \"/\" in ds_name:\n",
    "      ds_key = ds_name.split(\"/\")[0]\n",
    "      ds_freq = ds_name.split(\"/\")[1]\n",
    "      ds_key = ds_key.lower()\n",
    "      ds_key = pretty_names.get(ds_key, ds_key)\n",
    "    else:\n",
    "      ds_key = ds_name.lower()\n",
    "      ds_key = pretty_names.get(ds_key, ds_key)\n",
    "      ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "    ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "    # Initialize the dataset\n",
    "    to_univariate = (False if Dataset(\n",
    "        name=ds_name, term=term, to_univariate=False).target_dim == 1 else True)\n",
    "    dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "    all_ds_tuples.append(\n",
    "        (dataset.prediction_length, ds_config, ds_name, to_univariate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds_tuples = sorted(all_ds_tuples)\n",
    "all_ds_tuples[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on all settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"timesfm_2_0_500m\"\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "\n",
    "  # Write the header\n",
    "  writer.writerow([\n",
    "      \"dataset\",\n",
    "      \"model\",\n",
    "      \"eval_metrics/MSE[mean]\",\n",
    "      \"eval_metrics/MSE[0.5]\",\n",
    "      \"eval_metrics/MAE[0.5]\",\n",
    "      \"eval_metrics/MASE[0.5]\",\n",
    "      \"eval_metrics/MAPE[0.5]\",\n",
    "      \"eval_metrics/sMAPE[0.5]\",\n",
    "      \"eval_metrics/MSIS\",\n",
    "      \"eval_metrics/RMSE[mean]\",\n",
    "      \"eval_metrics/NRMSE[mean]\",\n",
    "      \"eval_metrics/ND[0.5]\",\n",
    "      \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "      \"domain\",\n",
    "      \"num_variates\",\n",
    "  ])\n",
    "\n",
    "for entry in all_ds_tuples:\n",
    "  prediction_length = entry[0]\n",
    "  ds_name = entry[2]\n",
    "  to_univariate = entry[3]\n",
    "  ds_config = entry[1]\n",
    "  ds_key, ds_freq, term = ds_config.split(\"/\")\n",
    "  dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "  season_length = get_seasonality(dataset.freq)\n",
    "  print(f\"Processing entry: {entry}\")\n",
    "  print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "  predictor = TimesFmPredictor(\n",
    "      tfm=tfm,\n",
    "      prediction_length=dataset.prediction_length,\n",
    "      ds_freq=ds_freq,\n",
    "  )\n",
    "  # Measure the time taken for evaluation\n",
    "  res = evaluate_model(\n",
    "      predictor,\n",
    "      test_data=dataset.test_data,\n",
    "      metrics=metrics,\n",
    "      batch_size=1024,\n",
    "      axis=None,\n",
    "      mask_invalid_label=True,\n",
    "      allow_nan_forecast=False,\n",
    "      seasonality=season_length,\n",
    "  )\n",
    "\n",
    "  # Append the results to the CSV file\n",
    "  with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\n",
    "        ds_config,\n",
    "        model_name,\n",
    "        res[\"MSE[mean]\"][0],\n",
    "        res[\"MSE[0.5]\"][0],\n",
    "        res[\"MAE[0.5]\"][0],\n",
    "        res[\"MASE[0.5]\"][0],\n",
    "        res[\"MAPE[0.5]\"][0],\n",
    "        res[\"sMAPE[0.5]\"][0],\n",
    "        res[\"MSIS\"][0],\n",
    "        res[\"RMSE[mean]\"][0],\n",
    "        res[\"NRMSE[mean]\"][0],\n",
    "        res[\"ND[0.5]\"][0],\n",
    "        res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "        dataset_properties_map[ds_key][\"domain\"],\n",
    "        dataset_properties_map[ds_key][\"num_variates\"],\n",
    "    ])\n",
    "\n",
    "  print(f\"Results for {ds_name} have been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/timesfm` folder containing the results for the Chronos model on the gift-eval benchmark. We can display the csv file using the follow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_16572\\761937587.py:10: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'ds': pd.date_range(start='2020-01-01', periods=144, freq='H'),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TimesFM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m context_batch \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mexpand_dims(jnp\u001b[38;5;241m.\u001b[39marray(context), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Inizializza il modello\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTimesFM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://amazon-science/timesfm-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Inferenza\u001b[39;00m\n\u001b[0;32m     32\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m generate_forecast(\n\u001b[0;32m     33\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     34\u001b[0m     input_array\u001b[38;5;241m=\u001b[39mcontext_batch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     }\n\u001b[0;32m     40\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TimesFM' is not defined"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "#from timesfm.models import TimesFM\n",
    "#from timesfm.inference import generate_forecast\n",
    "\n",
    "# Carica il tuo dataset\n",
    "#df = pd.read_csv(\"tuo_dataset.csv\")  # ad esempio con colonne 'ds', 'y'\n",
    "df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start='2020-01-01', periods=144, freq='H'),\n",
    "    'y': range(144)\n",
    "})\n",
    "\n",
    "# Supponiamo una serie univariata con date\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df = df.sort_values('ds')\n",
    "\n",
    "# Parametri del tuo task\n",
    "context_length = 96\n",
    "prediction_length = 48\n",
    "\n",
    "# Estrai le ultime `context_length` osservazioni\n",
    "context = df['y'].values[-context_length:]\n",
    "\n",
    "# TimesFM vuole un batch, quindi aggiungiamo un asse batch (1 serie)\n",
    "context_batch = jnp.expand_dims(jnp.array(context), axis=0)\n",
    "\n",
    "# Inizializza il modello\n",
    "model = TimesFM.from_pretrained(\"hf://amazon-science/timesfm-1.0\")\n",
    "\n",
    "# Inferenza\n",
    "forecasts = generate_forecast(\n",
    "    model=model,\n",
    "    input_array=context_batch,\n",
    "    config={\n",
    "        \"context_length\": context_length,\n",
    "        \"prediction_length\": prediction_length,\n",
    "        \"num_samples\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output: array di shape [batch, num_samples, prediction_length]\n",
    "forecast = forecasts[0, 0]  # prima serie, primo campione\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
